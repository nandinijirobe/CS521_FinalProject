{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPbff3S+4/VLL/cwM47LeDG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandinijirobe/CS521_FinalProject/blob/main/Copy_of_Final_CS_521_Research_Project_4_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Packages**"
      ],
      "metadata": {
        "id": "1WufGoiPdnmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install convokit # used to access reddit corpus\n",
        "!pip install codeswitch # nlp tool for hinglish language identification, pos tagging\n",
        "!pip install cleantext # used to clean raw text data\n",
        "!pip install demoji # used to remove presence of emojis from text"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LIeJwxy9Z2PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get Reddit Repo Lists**"
      ],
      "metadata": {
        "id": "VsnO8KVMdrTR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdyStz62ZhRJ"
      },
      "outputs": [],
      "source": [
        "# Allows us to read/write files that are present in personal drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# The subreddits_file contains a list of subreddits relevant to India and Hindi\n",
        "subreddits_file = open(r'/content/drive/MyDrive/CS521_FinalProject/Resources/unfamiliar_indian_subreddits.txt', 'r')\n",
        "\n",
        "# The below code creates a dataframe containing information about each subreddit\n",
        "# and the number of utterances it contains\n",
        "data = []\n",
        "for line in subreddits_file:\n",
        "  subreddit = line[0:line.index(\".corpus\")]\n",
        "  count = int(line[line.index(\":\")+3:].strip())\n",
        "  # print(subreddit, count)\n",
        "  data.append([subreddit, count])\n",
        "\n",
        "subreddits_df = pd.DataFrame(data, columns=['subreddits', 'num_utterances'])\n",
        "subreddits_df = subreddits_df.sort_values(by=['num_utterances'], ascending=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zdAvvW1Iay3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subreddits_df.head(10)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BxNsWP04c2ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total utterances across all subreddits:\", subreddits_df['num_utterances'].sum())\n",
        "print(\"Total subreddits:\", len(subreddits_df))"
      ],
      "metadata": {
        "id": "l3yycVked4by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Merge All Dataframes**"
      ],
      "metadata": {
        "id": "rPSJWNHBecZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from convokit import Corpus, download\n",
        "\"\"\"\n",
        "The subreddit we will be working with for this project is IndiaSpeaks\n",
        "It is a general subreddit ews, entertainment, science & technology, sports, history\n",
        "& culture, economy and geopolitics related to India.\n",
        "\"\"\"\n",
        "master_corpus = Corpus(filename=download(\"subreddit-IndiaSpeaks\")) #download data from the IndiaSpeaks subreddit\n",
        "total_utter_count = len(master_corpus.utterances) # store the total number of utterances in this subreddit\n",
        "\n",
        "\"\"\"\n",
        "The below code merges the data from all India/Hindi related subreddits into one dataframe\n",
        "Uncomment only if enough compute power and storage is available\n",
        "Otherwise continue to work with IndiaSpeaks subreddit as it is large and general\n",
        "\"\"\"\n",
        "# for subreddit in all_subreddits:\n",
        "#   subreddit_name = \"subreddit-\" + subreddit\n",
        "#   print(\"Currently downloading\", subreddit_name, \"...\")\n",
        "#   corpus = Corpus(filename=download(subreddit_name))\n",
        "#   master_corpus =  Corpus.merge(master_corpus, corpus)\n",
        "#   total_utter_count += len(corpus.utterances)\n",
        "\n",
        "print(total_utter_count)"
      ],
      "metadata": {
        "id": "N6HmqEXWehCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning**"
      ],
      "metadata": {
        "id": "SPKXna3mn3S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from codeswitch.codeswitch import LanguageIdentification # import hinglish language identification tool\n",
        "lid = LanguageIdentification('hin-eng') # load model to classify words"
      ],
      "metadata": {
        "id": "-8BNzZlPn1gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cleantext # used to clean raw text data\n",
        "import demoji # used to remove presence of emojis\n",
        "cleaned = 0\n",
        "def clean_utterance(text):\n",
        "  global cleaned\n",
        "  print((cleaned/len(master_corpus)) * 100, \"% Cleaned\")\n",
        "  cleaned += 1\n",
        "  text = cleantext.clean(text, extra_spaces=True, lowercase=True, numbers=True, punct=True)\n",
        "  text = demoji.replace(text, \"\")\n",
        "  return text"
      ],
      "metadata": {
        "id": "tEhb3fCxoOl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract just the utterances from the entire corpus\n",
        "master_corpus = master_corpus.get_utterances_dataframe()\n",
        "# remove empty texts form dataframe\n",
        "master_corpus = master_corpus[(master_corpus[\"text\"] != \"[removed]\") & (master_corpus[\"text\"] != \"\") & (master_corpus[\"text\"] != \"[deleted]\")]"
      ],
      "metadata": {
        "id": "LQldd03YodSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean all utterances\n",
        "master_corpus[\"text\"] = master_corpus[\"text\"].apply(clean_utterance)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h0rIgCH8od7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove empty text again after cleaning existing text\n",
        "master_corpus = master_corpus[(master_corpus[\"text\"].str.strip() != \"\")]\n",
        "\n",
        "# Cleaning takes a while. Store this file as a csv file into drive so we won't have run code again\n",
        "master_corpus.to_csv(r'/content/drive/MyDrive/CS521_FinalProject/Resources/cleaned_corpus.csv')"
      ],
      "metadata": {
        "id": "7UXHUxEOqtBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Break Down Data**"
      ],
      "metadata": {
        "id": "_J7a23il0TEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the file into master corpus in case the runtime disconnected -- no need to run above cells\n",
        "import pandas as pd\n",
        "master_corpus = pd.read_csv(r'/content/drive/MyDrive/CS521_FinalProject/Resources/cleaned_corpus.csv')"
      ],
      "metadata": {
        "id": "SxtFCPpTr_91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# master_corpus (which contains IndiaSpeaks subreddit) still has a lot of data\n",
        "# working with all the data at once will give us runtime issues so we need to do\n",
        "# it in batches or make it smaller\n",
        "corpus_size = len(master_corpus)\n",
        "slice_size = corpus_size//30\n",
        "print(\"Corpus size:\", corpus_size)\n",
        "print(\"Slice size:\", slice_size)\n",
        "# We are going to be working with 15k texts specifically"
      ],
      "metadata": {
        "id": "W3t_Xl_AxEWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to slice the dataframe and perform batch processing so we don't get a runtime error from running the code for too long\n",
        "master_corpus1 = master_corpus.iloc[0:slice_size]\n",
        "# master_corpus2 = master_corpus.iloc[slice_count:slice_count*2]\n",
        "# master_corpus3 = master_corpus.iloc[slice_count*2:slice_count*3]\n",
        "# master_corpus4 = master_corpus.iloc[slice_count*3:total_utter_count]"
      ],
      "metadata": {
        "id": "c-abs4STswvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Filter out Hinglish Text**"
      ],
      "metadata": {
        "id": "F5Z0XCSYoSIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This function determines whether a piece of text is Hinglish.\n",
        "The condition for a text to qualify is that it should have at\n",
        "least 20% Hindi and 20% English.\n",
        "\"\"\"\n",
        "utter_checked = 1 # this keeps track of the number of texts the below function has checked if it is hinglish\n",
        "hinglish_found = 0 # this keeps track of the number of hinglish texts dounf\n",
        "\n",
        "def isHinglish (text):\n",
        "  global hinglish_found\n",
        "  global utter_checked\n",
        "  print(utter_checked/len(master_corpus) * 100, \"% Checked\")\n",
        "  utter_checked += 1\n",
        "  if isinstance(text, str) and text: # check if text is string and not null\n",
        "    word_list = text.split() # get the list of words in text\n",
        "\n",
        "    hin_count = 0\n",
        "    eng_count = 0\n",
        "\n",
        "    # categorize each word in text as either hindi or english\n",
        "    result = lid.identify(text)\n",
        "    for map in result:\n",
        "      if \"#\" in map['word']:\n",
        "        # This is from a splitted word.\n",
        "        # Already has been counted. Ignore.\n",
        "        continue\n",
        "      else:\n",
        "        if map['entity'] == \"hin\":\n",
        "          # print(\"hindi\", map['word'])\n",
        "          hin_count += 1\n",
        "        else:\n",
        "          # print(\"english\", map['word'])\n",
        "          eng_count += 1\n",
        "    # calculate the percentages of hindi and english text\n",
        "    hin_pct = round((hin_count/len(word_list))*100, 2)\n",
        "    eng_pct = round((eng_count/len(word_list))*100, 2)\n",
        "    # print(\"Hindi percentage: \", hin_pct)\n",
        "    # print(\"English percentage: \", eng_pct)\n",
        "\n",
        "    # A text will only be considered Hinglish if it contains\n",
        "    # 20% hindi and 20% english minimum\n",
        "    if (hin_pct >= 20 and eng_pct >= 20):\n",
        "      hinglish_found += 1\n",
        "      print(\"Hinglish found\", hinglish_found)\n",
        "    return (hin_pct >= 20 and eng_pct >= 20)"
      ],
      "metadata": {
        "id": "1r0TtYCnoROG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only keep hinglish text\n",
        "master_corpus1 = master_corpus1[master_corpus1[\"text\"].apply(isHinglish)]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Fk6h01G50KhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering out the hinglish text form master_corpus1 took a while. saving it to a csv so we won't have to redo it\n",
        "master_corpus1 = master_corpus1[\"text\"]\n",
        "master_corpus1.to_csv(\"/content/drive/MyDrive/CS521_FinalProject/Resources/master_corpus1_hinglish.csv\", index=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XyW6_Ata0l3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove Text with Devanagari Text**"
      ],
      "metadata": {
        "id": "uGb2x0ftaQD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "master_corpus1 = pd.read_csv(\"/content/drive/MyDrive/CS521_FinalProject/Resources/master_corpus1_hinglish.csv\")"
      ],
      "metadata": {
        "id": "2JEtyfUpjM4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure there is no text which contains devanagari text because we are only focusing on text written using roman script\n",
        "\"\"\"\n",
        "The devangari alphabet ranges from 0900 to 097F.\n",
        "The code below creates a string containing all\n",
        "the alphabets' unicodes.This was created to check\n",
        "if a text contained any devangari alphabets.\n",
        "\"\"\"\n",
        "import re\n",
        "devangariAlphabet = \"\"\n",
        "uniList = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
        "           \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
        "for i in range (90, 97):\n",
        "  for j in uniList:\n",
        "    devangariAlphabet += (\"\\\\u0\" + str(i) + str(j))\n",
        "devangariAlphabet = \"[\" + devangariAlphabet + \"]\"\n",
        "\n",
        "# checks if text contains and devangari alphabets\n",
        "def hasDevangariAlpha(text):\n",
        "  pattern = re.compile(devangariAlphabet)\n",
        "  # findall returns a tuple of the matches found\n",
        "  matches = pattern.findall(text)\n",
        "  if (len(matches) > 0):\n",
        "    return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "gPzF4KnTaFJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove any text that contains devangari letters from the data\n",
        "master_corpus1 = master_corpus1[~master_corpus1[\"text\"].apply(hasDevangariAlpha)]"
      ],
      "metadata": {
        "id": "3HVboNo4ae9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# update the saved csv file again\n",
        "master_corpus1.to_csv(\"/content/drive/MyDrive/CS521_FinalProject/Resources/master_corpus1_hinglish.csv\", index=False)"
      ],
      "metadata": {
        "id": "e0W0X-PuptBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **POS Tagging**"
      ],
      "metadata": {
        "id": "mNXii6UWM03C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "master_corpus1.head(5)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OmSzVszg0nP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from codeswitch.codeswitch import POS\n",
        "pos = POS('hin-eng')\n",
        "from codeswitch.codeswitch import LanguageIdentification\n",
        "lid = LanguageIdentification('hin-eng') # load model to classify words"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B8n4B5HyfuVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This function splits each sentence into tokens and POS tags each token.\n",
        "It keeps track of total Hindi and English counts for each POS tag and updates\n",
        "global dictionaries. It is used to figure out when a person is more likely to code\n",
        "switch to another language.\n",
        "\"\"\"\n",
        "eng_pos_counts = {} # will contain pos tag counts for all english words\n",
        "hin_pos_counts = {}# will contain pos tag counts for all hindi words\n",
        "hin_total = 0 # will store the total number of hindi words\n",
        "eng_total = 0 # will store the total number of english words\n",
        "total_word_count = 0\n",
        "total_pos_counts = {} # will contain pos tag counts for all words\n",
        "utter_count = 0\n",
        "def pos_tagger (text):\n",
        "  global total_word_count\n",
        "  global hin_total\n",
        "  global eng_total\n",
        "  global utter_count\n",
        "  word_list = text.split()\n",
        "  lid_result = lid.identify(text) # give each word in text hindi/english tag\n",
        "  pos_result = pos.tag(text) # give each word in text pos tag\n",
        "  utter_count += 1 # keeps track of the number of texts function has processed\n",
        "  print(((utter_count/len(master_corpus1))*100), \"% has been POS tagged...\")\n",
        "\n",
        "  # Store the POS tag counts for text based on language\n",
        "  for i in range(len(word_list)):\n",
        "    if \"#\" in lid_result[i]['word']:\n",
        "      # This is from a splitted word.\n",
        "      # Already has been counted. Ignore.\n",
        "      continue\n",
        "    else:\n",
        "      if pos_result[i]['entity'] in total_pos_counts: # update total pos tags container\n",
        "        total_pos_counts[pos_result[i]['entity']] += 1\n",
        "      else:\n",
        "        total_pos_counts[pos_result[i]['entity']] = 1\n",
        "\n",
        "      if lid_result[i]['entity'] == \"hin\": # update hindi pos tags container\n",
        "        hin_total += 1\n",
        "        if pos_result[i]['entity'] in hin_pos_counts:\n",
        "          hin_pos_counts[pos_result[i]['entity']] += 1\n",
        "        else:\n",
        "          hin_pos_counts[pos_result[i]['entity']] = 1\n",
        "      else:\n",
        "        eng_total += 1\n",
        "        if pos_result[i]['entity'] in eng_pos_counts: # update english pos tags container\n",
        "          eng_pos_counts[pos_result[i]['entity']] += 1\n",
        "        else:\n",
        "          eng_pos_counts[pos_result[i]['entity']] = 1\n",
        "\n",
        "  total_word_count += len(word_list)\n",
        "  return\n"
      ],
      "metadata": {
        "id": "caGhgEjWZ1xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_corpus1[\"text\"] = master_corpus1[\"text\"].apply(lambda x: x.replace(\"\\n\", \" \")) # needed to do some extra cleaning\n",
        "master_corpus1[\"text\"].apply(pos_tagger) # start calculating the pos tags for all texts in dataframe"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BpribcsEdSrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_corpus1.head(20)"
      ],
      "metadata": {
        "id": "zzkhtL5wkdPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing Data for Graphing**"
      ],
      "metadata": {
        "id": "4qwqcpgddcWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# The dictionaries created above for each language's pos_counts need to be used to create\n",
        "# graphs. However they need to be made into dataframes if we want to use the seaborn library\n",
        "# The below code combines the dictionaries and makes a dataframe\n",
        "# make these dictionaries into a dataframe so we can save results as a csv and also use it make seaborn graphs\n",
        "def two_dicts_to_df(dict1, dict2, total_dict, tag_type):\n",
        "  tags = total_pos_counts.keys()\n",
        "  # ensure both dictionaries have same number of keys\n",
        "  for tag in tags:\n",
        "    if tag not in dict1:\n",
        "      dict1[tag] = 0\n",
        "    if tag not in dict2:\n",
        "      dict2[tag] = 0\n",
        "\n",
        "  # sort dictionaries based on keys\n",
        "  dict1 = dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n",
        "  dict2 = dict(sorted(dict2.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "  data = {tag_type + \"_tags\":[], \"language\":[], \"count\":[]} # create columns for dataframe\n",
        "  for tag in tags: # add information that will be stored in dictionary\n",
        "    data[tag_type + \"_tags\"].append(tag)\n",
        "    data[\"language\"].append(\"English\")\n",
        "    data[\"count\"].append(int((dict1[tag]/total_dict[tag]) * 100))\n",
        "\n",
        "    data[tag_type + \"_tags\"].append(tag)\n",
        "    data[\"language\"].append(\"Hindi\")\n",
        "    data[\"count\"].append(int((dict2[tag]/total_dict[tag]) * 100))\n",
        "\n",
        "  df = pd.DataFrame(data) # create the dataframe\n",
        "  return df\n",
        "\n",
        "pos_df = two_dicts_to_df(eng_pos_counts, hin_pos_counts, total_pos_counts, \"pos\")\n",
        "pos_df.head(20)"
      ],
      "metadata": {
        "id": "5N4d6piedVAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results shown as Graph**"
      ],
      "metadata": {
        "id": "ETiG5jowdlZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"dark\")\n",
        "\n",
        "g = sns.catplot(\n",
        "    data=pos_df, kind=\"bar\",\n",
        "    x=\"pos_tags\", y=\"count\", hue=\"language\",\n",
        "    errorbar=\"sd\", palette=\"dark\", alpha=.6, height=5, aspect=2\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_xticklabels(rotation=20)\n",
        "g.set_axis_labels(\"POS Tags\", \"% Out of Total Tags\")\n",
        "g.legend.set_title(\"Distribution of POS Tags by Language\")\n",
        "sns.move_legend(g, \"upper left\", bbox_to_anchor=(0.9, 1))"
      ],
      "metadata": {
        "id": "5r-OYgRKdYLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modelling: LDA**\n",
        "**Note**: I wasn't familiar with doing LDA topic modelling this so followed a tutorial to write the below code: https://www.youtube.com/watch?v=b91ohJvEst4"
      ],
      "metadata": {
        "id": "f2I4cu13pErq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "LtGZQ3VYxsig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_corpus1 = pd.read_csv(\"/content/drive/MyDrive/CS521_FinalProject/Resources/master_corpus1_hinglish.csv\") # reload master_corpus1 again"
      ],
      "metadata": {
        "id": "BWfuvQJ43beP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# hinglish stop words we don't want to include in the topic modelling\n",
        "hindi_sw = ['ka', 'nahi', 'ki', 'hai', 'ke', 'ho', 'kya', 'bhi', 'aa', 'toh', 'ko', 'ye', 'gaya']\n",
        "hinglish_stop_words = list(text.ENGLISH_STOP_WORDS.union(hindi_sw))\n",
        "\n",
        "# min_df specifies we want to keep words that show up in at least 5 docs/texts\n",
        "# max_df specifies we want to keep words that show up in at most 75% or less docs/texts. This helps us exclude common words\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words = hinglish_stop_words, min_df = 5, max_df = 0.75)\n",
        "\n",
        "# tokenize the data, remove stop words, remove most common words, and return what is left\n",
        "doc_term_matrix = tfidf_vectorizer.fit_transform(master_corpus1['text'])\n",
        "\n",
        "print(f'Rows: {doc_term_matrix. shape[0]}, Columns: {doc_term_matrix. shape[1]}' )"
      ],
      "metadata": {
        "id": "dk4-G-1x377f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn. decomposition import LatentDirichletAllocation\n",
        "\n",
        "num_topics = 3 # this is the number of topics we are trying to find all the texts\n",
        "lda_topic_model = LatentDirichletAllocation(n_components = num_topics, random_state = 12345)\n",
        "doc_topic_matrix = lda_topic_model. fit_transform(doc_term_matrix) # train and get texts' topic assignments\n",
        "col_names = [f'Topic {x}' for x in range(1, num_topics + 1)]\n",
        "doc_topic_df = pd.DataFrame(doc_topic_matrix, columns = col_names) # display each document's topic assignments\n",
        "doc_topic_df.head(n = 10)"
      ],
      "metadata": {
        "id": "yr4UjArh7VNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modelling Results**"
      ],
      "metadata": {
        "id": "xMkcfW4kefB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 10 # the top words we want to display for each topic\n",
        "\n",
        "# The below code gives us the top words for each topic\n",
        "for topic, words in enumerate(lda_topic_model. components_):\n",
        "  word_total = words.sum()\n",
        "  sorted_words = words.argsort() [ ::- 1]\n",
        "  print(f'\\nTopic {topic + 1:02d}')\n",
        "  for i in range(0, num_words):\n",
        "    word = tfidf_vectorizer.get_feature_names_out() [sorted_words[i]]\n",
        "    word_weight = words[sorted_words[i]]\n",
        "    print(f'{word} ({word_weight :.3f})')"
      ],
      "metadata": {
        "id": "xebwqEtv7z7H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}